{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master jupyter notebook for Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime\n",
    "from forex_python.converter import CurrencyRates\n",
    "from forex_python.converter import RatesNotAvailableError\n",
    "import plotly.express as px\n",
    "import nbformat\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting tables from database\n",
    "def list_tables(database_file):\n",
    "    connection = sqlite3.connect('data.db')\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"\"\"select name from sqlite_master where type = 'table';\n",
    "                    \"\"\")\n",
    "                    \n",
    "    tables = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    table_names = [table[0] for table in tables]\n",
    "    return table_names\n",
    "\n",
    "# saving tables as dataframes\n",
    "connection = sqlite3.connect('data.db')\n",
    "query = \"\"\"select *\n",
    "        from client as cl;\n",
    "        \"\"\"\n",
    "client = pd.read_sql_query(query, connection)\n",
    "connection.close\n",
    "\n",
    "connection = sqlite3.connect('data.db')\n",
    "query = \"\"\"select *\n",
    "        from client_products as cl;\n",
    "        \"\"\"\n",
    "client_products = pd.read_sql_query(query, connection)\n",
    "connection.close\n",
    "\n",
    "connection = sqlite3.connect('data.db')\n",
    "query = \"\"\"select *\n",
    "        from balances as cl;\n",
    "        \"\"\"\n",
    "balances = pd.read_sql_query(query, connection)\n",
    "connection.close\n",
    "\n",
    "connection = sqlite3.connect('data.db')\n",
    "query = \"\"\"select *\n",
    "        from inv_campaign_eval as cl;\n",
    "        \"\"\"\n",
    "inv_campaign_eval = pd.read_sql_query(query, connection)\n",
    "connection.close\n",
    "\n",
    "# merging tables with client data (excluding balances and campaign eval table)\n",
    "connection = sqlite3.connect('data.db')\n",
    "query = \"\"\"\n",
    "        select cl.client_id, age, job, marital, education, gender, has_deposits, loan, has_insurance, has_mortgage\n",
    "        from client as cl\n",
    "        left join client_products as cp on cl.client_id = cp.client_id\n",
    "        --left join balances as ba on cl.client_id = ba.client_id\n",
    "        --left join inv_campaign_eval as camp on cl.client_id = camp.client_id;\n",
    "        \"\"\"\n",
    "clients_merged = pd.read_sql_query(query, connection)\n",
    "connection.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for datatypes\n",
    "print(clients_merged.dtypes)\n",
    "\n",
    "# checking for missing values\n",
    "print(client.isnull().sum()) # missing values in Age and Job\n",
    "print(client_products.isnull().sum()) # no missing values\n",
    "print(inv_campaign_eval.isnull().sum()) # no missing values\n",
    "print(balances.isnull().sum()) # no missing values\n",
    "\n",
    "# dealing with missing values\n",
    "clients_merged['job'].fillna('unknown', inplace=True) # imputing \"unknown\" in job\n",
    "clients_merged['age'].fillna(clients_merged['age'].median(), inplace=True) # imputing median in age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding categorical variables\n",
    "le = LabelEncoder()\n",
    "clients_merged['has_deposits_enc'] = le.fit_transform(clients_merged['has_deposits'])\n",
    "clients_merged['loan_enc'] = le.fit_transform(clients_merged['loan'])\n",
    "clients_merged['has_insurance_enc'] = le.fit_transform(clients_merged['has_insurance'])\n",
    "clients_merged['has_mortgage_enc'] = le.fit_transform(clients_merged['has_mortgage'])\n",
    "clients_merged['marital_enc'] = le.fit_transform(clients_merged['marital'])\n",
    "clients_merged['education_enc'] = le.fit_transform(clients_merged['education'])\n",
    "clients_merged['gender_enc'] = le.fit_transform(clients_merged['gender'])\n",
    "clients_merged['job_enc'] = le.fit_transform(clients_merged['job'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting encoding labels for future reference\n",
    "job_labels = clients_merged[['job', 'job_enc']].drop_duplicates().sort_values('job_enc')\n",
    "gender_labels = clients_merged[['gender', 'gender_enc']].drop_duplicates().sort_values('gender_enc')\n",
    "education_labels = clients_merged[['education', 'education_enc']].drop_duplicates().sort_values('education_enc')\n",
    "marital_labels = clients_merged[['marital', 'marital_enc']].drop_duplicates().sort_values('marital_enc')\n",
    "has_mortgage_labels = clients_merged[['has_mortgage', 'has_mortgage_enc']].drop_duplicates().sort_values('has_mortgage_enc')\n",
    "has_insurance_labels = clients_merged[['has_insurance', 'has_insurance_enc']].drop_duplicates().sort_values('has_insurance_enc')\n",
    "loan_labels = clients_merged[['loan', 'loan_enc']].drop_duplicates().sort_values('loan_enc')\n",
    "has_deposits_labels = clients_merged[['has_deposits', 'has_deposits_enc']].drop_duplicates().sort_values('has_deposits_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editing table balances\n",
    "# removing time from datetime\n",
    "balances['date'] = pd.to_datetime(balances['date'])\n",
    "balances['date'] = balances['date'].dt.date\n",
    "\n",
    "# fetching currency exchange rates for EUR and USD\n",
    "c = CurrencyRates()\n",
    "\n",
    "dates = balances['date'].drop_duplicates()\n",
    "target_currency = 'CZK'\n",
    "\n",
    "# EUR\n",
    "base_currency = 'EUR'\n",
    "\n",
    "eur_rates_date = []\n",
    "eur_rates_rate = []\n",
    "\n",
    "for date in dates:\n",
    "    rate = c.get_rate(base_currency, target_currency, date)\n",
    "    eur_rates_date.append(date)\n",
    "    eur_rates_rate.append(rate)\n",
    "\n",
    "eur_df = pd.DataFrame({'date': eur_rates_date, 'eur_rate': eur_rates_rate})\n",
    "\n",
    "# USD\n",
    "base_currency = 'USD'\n",
    "\n",
    "usd_rates_date = []\n",
    "usd_rates_rate = []\n",
    "\n",
    "for date in dates:\n",
    "    rate = c.get_rate(base_currency, target_currency, date)\n",
    "    usd_rates_date.append(date)\n",
    "    usd_rates_rate.append(rate)\n",
    "\n",
    "usd_df = pd.DataFrame({'date': usd_rates_date, 'usd_rate': usd_rates_rate})\n",
    "\n",
    "# mapping exchange rate columns into balances dataframe\n",
    "balances = pd.merge(balances, eur_df, on = 'date')\n",
    "balances = pd.merge(balances, usd_df, on = 'date')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column with balances recalculated to czk\n",
    "for index, row in balances.iterrows():\n",
    "    if row['currency'] == 'EUR':\n",
    "        balances.at[index, 'fx_rate'] = balances.at[index, 'eur_rate']\n",
    "    elif row['currency'] == 'USD':\n",
    "        balances.at[index, 'fx_rate'] = balances.at[index, 'usd_rate']\n",
    "    else:\n",
    "        balances.at[index, 'fx_rate'] = 1\n",
    "\n",
    "balances['balance_in_czk'] = balances['balance']*balances['fx_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table balances: editing and feature engineering from variables: balance, currency\n",
    "# encoding outcome variable \n",
    "inv_campaign_eval['poutcome_enc'] = le.fit_transform(inv_campaign_eval['poutcome'])\n",
    "\n",
    "# last balance\n",
    "last_timestamp_index = balances.groupby('client_id')['date'].idxmax()\n",
    "last_timestamp_df = balances.loc[last_timestamp_index]\n",
    "last_timestamp_df.rename(columns={'balance_in_czk': 'last_balance'}, inplace=True)\n",
    "clients_merged = pd.merge(clients_merged, last_timestamp_df[['last_balance','client_id']], on = 'client_id')\n",
    "\n",
    "# mean balance\n",
    "mean_bal_by_client = balances.groupby('client_id')['balance_in_czk'].mean().reset_index()\n",
    "mean_bal_by_client.rename(columns={'balance_in_czk': 'mean_balance'}, inplace=True)\n",
    "clients_merged = pd.merge(clients_merged, mean_bal_by_client[['mean_balance','client_id']], on = 'client_id')\n",
    "\n",
    "# min balance\n",
    "min_bal_by_client = balances.groupby('client_id')['balance_in_czk'].min().reset_index()\n",
    "min_bal_by_client.rename(columns={'balance_in_czk': 'min_balance'}, inplace=True)\n",
    "clients_merged = pd.merge(clients_merged, min_bal_by_client[['min_balance','client_id']], on = 'client_id')\n",
    "\n",
    "# max balance\n",
    "max_bal_by_client = balances.groupby('client_id')['balance_in_czk'].max().reset_index()\n",
    "max_bal_by_client.rename(columns={'balance_in_czk': 'max_balance'}, inplace=True)\n",
    "clients_merged = pd.merge(clients_merged, max_bal_by_client[['max_balance','client_id']], on = 'client_id')\n",
    "\n",
    "# currency (encoded)\n",
    "clients_merged = pd.merge(clients_merged, balances[['client_id','currency']].drop_duplicates(), on = 'client_id')\n",
    "clients_merged['currency_enc'] = le.fit_transform(clients_merged['currency'])\n",
    "currency_labels = clients_merged[['currency', 'currency_enc']].drop_duplicates().sort_values('currency_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating table first round of campaign - original columns (for visualizations)\n",
    "first_round_vis = clients_merged[['client_id', 'age', 'job', 'marital', 'education', 'gender','has_deposits', 'loan', 'has_insurance', 'has_mortgage', 'last_balance','mean_balance', 'min_balance', 'max_balance', 'currency']]\n",
    "first_round_vis = pd.merge(inv_campaign_eval[['client_id','poutcome']], first_round_vis, on = 'client_id')\n",
    "first_round_vis['age'] = first_round_vis['age'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations of data from first round of campaign\n",
    "# TO DO\n",
    "\n",
    "plt.figure(figsize=(40, 15))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "sns.countplot(x='job', hue='poutcome', data=first_round_vis, palette='viridis')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "sns.countplot(x='gender', hue='poutcome', data=first_round_vis, palette='viridis')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "sns.countplot(x='education', hue='poutcome', data=first_round_vis, palette='viridis')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "sns.countplot(x='marital', hue='poutcome', data=first_round_vis, palette='viridis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# ------------------------------------------------------------------------------------------\n",
    "plt.figure(figsize=(40, 15))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "sns.countplot(x='has_deposits', hue='poutcome', data=first_round_vis, palette='viridis')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "sns.countplot(x='loan', hue='poutcome', data=first_round_vis, palette='viridis')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "sns.countplot(x='has_insurance', hue='poutcome', data=first_round_vis, palette='viridis')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "sns.countplot(x='has_mortgage', hue='poutcome', data=first_round_vis, palette='viridis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating table first round of campaign - encoded columns (for modeling)\n",
    "first_round_clients = clients_merged[['client_id', 'age', 'has_deposits_enc', 'loan_enc', 'has_insurance_enc', 'has_mortgage_enc','marital_enc', 'education_enc', 'gender_enc', 'job_enc', 'last_balance','mean_balance', 'min_balance', 'max_balance', 'currency_enc']]\n",
    "first_round_clients = pd.merge(inv_campaign_eval[['client_id','poutcome_enc']], first_round_clients, on = 'client_id')\n",
    "first_round_clients['age'] = first_round_clients['age'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating X-matrix of predictors and y-outcome variable\n",
    "X = first_round_clients.drop(['client_id','poutcome_enc'], axis = 1)\n",
    "y = first_round_clients['poutcome_enc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting most important features from X using feature importance\n",
    "classifier = DecisionTreeClassifier(max_depth=10, min_samples_split=4)\n",
    "\n",
    "classifier.fit(X, y)\n",
    "importances = classifier.feature_importances_\n",
    "feature_names = ['age', 'has_deposits_enc', 'loan_enc', 'has_insurance_enc', 'has_mortgage_enc',\n",
    "                     'marital_enc', 'education_enc', 'gender_enc', 'job_enc', 'last_balance',\n",
    "                     'mean_balance', 'min_balance', 'max_balance', 'currency_enc']\n",
    "\n",
    "sorted_data = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse = True)\n",
    "sorted_feature_names, sorted_importances = zip(*sorted_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Feature Importances - Decision Tree')\n",
    "plt.bar(sorted_feature_names, sorted_importances, align=\"center\")\n",
    "plt.xticks(sorted_feature_names, rotation=90)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()\n",
    "\n",
    "# based on decision tree and feature importances, we consider as most important these variables:\n",
    "# 'has_deposits_enc', 'age', 'last_balance','min_balance', 'has_mortgage_enc','max_balance','mean_balance','job_enc'\n",
    "X = X[['has_deposits_enc', 'age', 'last_balance','min_balance', 'has_mortgage_enc','max_balance','mean_balance','job_enc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing train-test split and scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on Test data - compare models on F1, AUC - choose best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editing data of clients not included in first run\n",
    "# Second_round_clients poutcome_enc\n",
    "not_in_first_round = first_round_clients[['client_id','poutcome_enc']]\n",
    "merged_df = pd.merge(not_in_first_round, clients_merged, on='client_id', how='outer', indicator=True, suffixes=('', ' '))\n",
    "\n",
    "# # Select rows where 'client_id' is not present in 'first_round_clients'\n",
    "merged_df = merged_df[merged_df['_merge'] == 'right_only']\n",
    "\n",
    "# Drop the '_merge' column (optional)\n",
    "not_in_first_round = merged_df.drop('_merge', axis=1)\n",
    "not_in_first_round = not_in_first_round[['client_id','poutcome_enc', 'age', 'has_deposits_enc', 'loan_enc', 'has_insurance_enc', 'has_mortgage_enc','marital_enc', 'education_enc', 'gender_enc', 'job_enc', 'last_balance','mean_balance', 'min_balance', 'max_balance', 'currency_enc']]\n",
    "# Display the result\n",
    "not_in_first_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run best model on data not included in first run - output predict_proba - select 3000 ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
